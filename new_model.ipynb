{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional, LSTM, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_paths):\n",
    "    dfs = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    labels_to_remove = ['DictionaryBruteForce', 'BrowserHijacking', 'XSS', 'Uploading_Attack', 'SqlInjection', 'CommandInjection', 'Backdoor_Malware']\n",
    "    df = df[~df['label'].isin(labels_to_remove)]\n",
    "    \n",
    "    # To check the distribution of labels\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "\n",
    "    \n",
    "    columns_to_drop = ['label', 'flow_id', 'src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'timestamp']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "    \n",
    "    X = df.drop(columns_to_drop, axis=1)\n",
    "    y = df['label']\n",
    "    \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    \n",
    "    return X, y, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(X_train, y_train, X_val, y_val, num_classes):\n",
    "\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(256, kernel_size=3, activation='relu'),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')  \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=5,\n",
    "                        batch_size=32,\n",
    "                        verbose=1)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, le):\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_classes, y_pred_classes, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, le, model_dir='saved_model'):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    model.save(os.path.join(model_dir, 'ddos_model.h5'))\n",
    "    joblib.dump(scaler, os.path.join(model_dir, 'scaler.joblib'))\n",
    "    joblib.dump(le, os.path.join(model_dir, 'label_encoder.joblib'))\n",
    "    print(f\"Model and associated objects saved in {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model_dir='saved_model'):\n",
    "    model = load_model(os.path.join(model_dir, 'ddos_model.h5'))\n",
    "    scaler = joblib.load(os.path.join(model_dir, 'scaler.joblib'))\n",
    "    le = joblib.load(os.path.join(model_dir, 'label_encoder.joblib'))\n",
    "    print(f\"Model and associated objects loaded from {model_dir}\")\n",
    "    return model, scaler, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loaded_model(model, scaler, le, X_test, y_test):\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=len(le.classes_))\n",
    "    \n",
    "    print(\"Evaluating loaded model:\")\n",
    "    evaluate_model(model, X_test_scaled, y_test_cat, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "DDoS-ICMP_Flood            544992\n",
      "DDoS-UDP_Flood             409362\n",
      "DDoS-TCP_Flood             340918\n",
      "DDoS-PSHACK_Flood          310418\n",
      "DDoS-SYN_Flood             307639\n",
      "DDoS-RSTFINFlood           305877\n",
      "DDoS-SynonymousIP_Flood    271629\n",
      "DoS-UDP_Flood              251542\n",
      "DoS-TCP_Flood              202278\n",
      "DoS-SYN_Flood              152378\n",
      "BenignTraffic               83268\n",
      "Mirai-greeth_flood          74557\n",
      "Mirai-udpplain              67454\n",
      "Mirai-greip_flood           57035\n",
      "DDoS-ICMP_Fragmentation     34273\n",
      "MITM-ArpSpoofing            23399\n",
      "DDoS-UDP_Fragmentation      21861\n",
      "DDoS-ACK_Fragmentation      21759\n",
      "DNS_Spoofing                13586\n",
      "Recon-HostDiscovery         10096\n",
      "Recon-OSScan                 7600\n",
      "Recon-PortScan               6172\n",
      "DoS-HTTP_Flood               5506\n",
      "VulnerabilityScan            2848\n",
      "DDoS-HTTP_Flood              2163\n",
      "DDoS-SlowLoris               1789\n",
      "Recon-PingSweep               139\n",
      "Name: count, dtype: int64\n",
      "Features: ['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight']\n",
      "Number of features: 46\n",
      "Unique labels: ['BenignTraffic' 'DDoS-ACK_Fragmentation' 'DDoS-HTTP_Flood'\n",
      " 'DDoS-ICMP_Flood' 'DDoS-ICMP_Fragmentation' 'DDoS-PSHACK_Flood'\n",
      " 'DDoS-RSTFINFlood' 'DDoS-SYN_Flood' 'DDoS-SlowLoris'\n",
      " 'DDoS-SynonymousIP_Flood' 'DDoS-TCP_Flood' 'DDoS-UDP_Flood'\n",
      " 'DDoS-UDP_Fragmentation' 'DNS_Spoofing' 'DoS-HTTP_Flood' 'DoS-SYN_Flood'\n",
      " 'DoS-TCP_Flood' 'DoS-UDP_Flood' 'MITM-ArpSpoofing' 'Mirai-greeth_flood'\n",
      " 'Mirai-greip_flood' 'Mirai-udpplain' 'Recon-HostDiscovery' 'Recon-OSScan'\n",
      " 'Recon-PingSweep' 'Recon-PortScan' 'VulnerabilityScan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ddos\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m70611/70611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m854s\u001b[0m 12ms/step - accuracy: 0.8753 - loss: 0.3537 - val_accuracy: 0.9669 - val_loss: 0.1036\n",
      "Epoch 2/5\n",
      "\u001b[1m70611/70611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m824s\u001b[0m 12ms/step - accuracy: 0.9639 - loss: 0.1154 - val_accuracy: 0.9676 - val_loss: 0.1002\n",
      "Epoch 3/5\n",
      "\u001b[1m70611/70611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 9ms/step - accuracy: 0.9643 - loss: 0.1115 - val_accuracy: 0.9678 - val_loss: 0.0989\n",
      "Epoch 4/5\n",
      "\u001b[1m70611/70611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m856s\u001b[0m 12ms/step - accuracy: 0.9648 - loss: 0.1094 - val_accuracy: 0.9676 - val_loss: 0.0960\n",
      "Epoch 5/5\n",
      "\u001b[1m70611/70611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m845s\u001b[0m 12ms/step - accuracy: 0.9651 - loss: 0.1071 - val_accuracy: 0.9680 - val_loss: 0.0949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and associated objects saved in saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and associated objects loaded from saved_model\n",
      "Evaluating loaded model:\n",
      "\u001b[1m22066/22066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ddos\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9678\n",
      "Precision: 0.9696\n",
      "Recall: 0.9678\n",
      "F1-score: 0.9608\n",
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ddos\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\anaconda\\envs\\ddos\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "          BenignTraffic       0.74      0.98      0.84     16654\n",
      " DDoS-ACK_Fragmentation       1.00      0.97      0.98      4352\n",
      "        DDoS-HTTP_Flood       0.74      0.52      0.61       432\n",
      "        DDoS-ICMP_Flood       1.00      1.00      1.00    108998\n",
      "DDoS-ICMP_Fragmentation       0.99      0.98      0.98      6855\n",
      "      DDoS-PSHACK_Flood       1.00      1.00      1.00     62084\n",
      "       DDoS-RSTFINFlood       1.00      1.00      1.00     61175\n",
      "         DDoS-SYN_Flood       1.00      1.00      1.00     61528\n",
      "         DDoS-SlowLoris       0.51      0.32      0.39       358\n",
      "DDoS-SynonymousIP_Flood       1.00      1.00      1.00     54326\n",
      "         DDoS-TCP_Flood       1.00      1.00      1.00     68184\n",
      "         DDoS-UDP_Flood       1.00      1.00      1.00     81872\n",
      " DDoS-UDP_Fragmentation       0.99      0.98      0.98      4372\n",
      "           DNS_Spoofing       0.40      0.37      0.38      2717\n",
      "         DoS-HTTP_Flood       0.78      0.92      0.84      1101\n",
      "          DoS-SYN_Flood       1.00      0.99      0.99     30476\n",
      "          DoS-TCP_Flood       1.00      0.99      1.00     40456\n",
      "          DoS-UDP_Flood       0.99      1.00      0.99     50308\n",
      "       MITM-ArpSpoofing       0.89      0.44      0.59      4680\n",
      "     Mirai-greeth_flood       0.57      0.99      0.72     14911\n",
      "      Mirai-greip_flood       0.64      0.00      0.00     11407\n",
      "         Mirai-udpplain       1.00      0.99      0.99     13491\n",
      "    Recon-HostDiscovery       0.72      0.52      0.60      2019\n",
      "           Recon-OSScan       0.35      0.02      0.04      1520\n",
      "        Recon-PingSweep       0.00      0.00      0.00        28\n",
      "         Recon-PortScan       0.49      0.23      0.31      1234\n",
      "      VulnerabilityScan       0.28      0.96      0.43       570\n",
      "\n",
      "               accuracy                           0.97    706108\n",
      "              macro avg       0.78      0.75      0.73    706108\n",
      "           weighted avg       0.97      0.97      0.96    706108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ddos\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # directory = \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\\"\n",
    "    \n",
    "    # # Use glob to find all CSV files that match the pattern\n",
    "    # file_paths = glob.glob(os.path.join(directory, \"part-*.csv\"))    \n",
    "\n",
    "    file_paths = [\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00001-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00002-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00003-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00004-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00005-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00006-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00007-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00008-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00009-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00010-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00011-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00012-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00013-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \"D:\\\\DDOS\\\\New\\\\archive(4)\\\\wataiData\\\\csv\\\\CICIoT2023\\\\part-00014-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\",\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    X, y, le = load_and_preprocess_data(file_paths)\n",
    "    \n",
    "    print(\"Features:\", X.columns.tolist())\n",
    "    print(\"Number of features:\", X.shape[1])\n",
    "    print(\"Unique labels:\", le.classes_)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    num_classes = len(np.unique(y))\n",
    "    y_train_cat = to_categorical(y_train, num_classes)\n",
    "    y_val_cat = to_categorical(y_val, num_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "    model, history = create_and_train_model(X_train_scaled, y_train_cat, X_val_scaled, y_val_cat, num_classes)\n",
    "    \n",
    "    save_model(model, scaler, le)   \n",
    "\n",
    "    loaded_model, loaded_scaler, loaded_le = load_saved_model()\n",
    "\n",
    "    test_loaded_model(loaded_model, loaded_scaler, loaded_le, X_test, y_test)                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "d:\\anaconda\\envs\\ddos\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 652ms/step\n",
      "Predicted label: BenignTraffic\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "def predict_ddos(input_data, model_dir='saved_model'):\n",
    "    model = load_model(f\"{model_dir}/ddos_model.h5\")\n",
    "    scaler = joblib.load(f\"{model_dir}/scaler.joblib\")\n",
    "    le = joblib.load(f\"{model_dir}/label_encoder.joblib\")\n",
    "    \n",
    "    # Ensure input_data is a 2D numpy array\n",
    "    if isinstance(input_data, pd.DataFrame):\n",
    "        input_data = input_data.values\n",
    "    elif isinstance(input_data, list):\n",
    "        input_data = np.array(input_data).reshape(1, -1)\n",
    "    elif isinstance(input_data, np.ndarray) and input_data.ndim == 1:\n",
    "        input_data = input_data.reshape(1, -1)\n",
    "    \n",
    "    # Scale the input data\n",
    "    input_data_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # Reshape for Conv1D layer\n",
    "    input_data_reshaped = input_data_scaled.reshape(input_data_scaled.shape[0], input_data_scaled.shape[1], 1)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_data_reshaped)\n",
    "    \n",
    "    # Get the predicted class label and probability\n",
    "    predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_probability = np.max(prediction)\n",
    "    predicted_label = le.inverse_transform([predicted_class_index])[0]\n",
    "    \n",
    "    return predicted_label, predicted_probability\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Make a prediction with sample data\n",
    "    # Replace this with actual feature values from your dataset\n",
    "    sample_data = [1000, 20, 6, 60, 100, 50, 50, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1000, 10, 100, 55, 30, 1000, 0.1, 10, 100, 50, 0.5, 900, 1]\n",
    "    \n",
    "    predicted_label, predicted_probability = predict_ddos(sample_data)\n",
    "    \n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "    # print(f\"Prediction probability: {predicted_probability:.4f}\")\n",
    "    \n",
    "    # Interactive prediction\n",
    "    # print(\"\\nEnter custom values for prediction (comma-separated):\")\n",
    "    # user_input = input(\"Enter values: \")\n",
    "    # user_values = [float(x.strip()) for x in user_input.split(',')]\n",
    "    \n",
    "    # custom_prediction, custom_probability = predict_ddos(user_values)\n",
    "    # print(f\"Predicted label for custom input: {custom_prediction}\")\n",
    "    # print(f\"Prediction probability: {custom_probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
